%===================================================================================================
\documentclass[]{elsarticle}
%===================================================================================================

\usepackage{amssymb}
\usepackage{graphicx} % Used for inserting pdf as graphics
\usepackage{subcaption}
%\usepackage{subfig}
\usepackage{float} % Used fo0r 'H' float option in figures
\usepackage{hyperref} % Used for creating a hyperlink to reference parts
\usepackage{textcomp}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{url}
\usepackage{color}
\usepackage{mathtools}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

%===================================================================================================

\let\proof\relax 				
\let\endproof\relax 
\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition2}{Definition}

\theoremstyle{definition}
\newtheorem{lemma}{Theorem}
%\newcommand{\keywords}[1]{\par\addvspace\baselineskip}
%\noindent\keywordname\enspace\ignorespaces#1}
\newcommand{\ra}{\rightarrow}
\newcommand{\se}{\text{ }}
\newcommand{\sm}{\text{:}}
\newcommand{\mn}{\text{-}}
\newcommand{\comment}[1]{}

%===================================================================================================
\begin{document}
%===================================================================================================

\begin{frontmatter}

\title{Homogenization of Spiking Neural P Systems}

\author[1]{Ren Tristan A. de la Cruz\corref{cor1}}
\ead{radelacruz@up.edu.ph}
\cortext[cor1]{Corresponding Author}
\author[1]{Francis George C. Cabarle}
\ead{fccabarle@up.edu.ph}
\author[2]{Author Three}
\ead{author3@mail.com}

\address[1]
{
Algorithms and Complexity Lab, 
Department of Computer Science, 
University of the Philippines Diliman,
1101, Quezon City, Philippines.
}
\address[2]{(address 2)}


\begin{abstract}
ABSTRACT
\end{abstract}

\begin{keyword}
WORD, WORD, WORD, WORD
\end{keyword}

\end{frontmatter}


% ================================================================================================= %
\section{Background}
% ================================================================================================= %

MEMBRANE COMPUTING

SNP

\cite{ionescu-2006-snp}
\cite{chen-2008-snp-e}

SNP VARIANTS

SNP NORMAL FORMS + HOMOGENEOUS

SNP HOMOGEN LITERATURE

SNP HOMOGENIZATION
Section \ref{sec-prelim}
Section \ref{sec-prelim-lang}
Section \ref{sec-prelim-snp}
Section \ref{sec-homo}
Section \ref{sec-homo-sm}
Section \ref{sec-homo-ops}
Section \ref{sec-homo-algo}
Section \ref{sec-rem-con}

% ================================================================================================= %
\section{Preliminaries} \label{sec-prelim}
% ================================================================================================= %

% ================================================================================================= %
\subsection{Languages and Regular Languages} \label{sec-prelim-lang}
% ================================================================================================= %

An alphabet $V$ is a finite set of symbols, a string $s$ is a concatenation of symbols from some $V$, so that the string is said to be \textit{over
the alphabet} $V$. If $s$ is a string over $V$, we denote as $|s|$ the length of $s$ while $|s|_a$ where $a \in V$ denotes the number of occurrences 
of symbol $a$ in $s$. 
  
A language $L$ is a set of strings. When talking about languages, the term \textit{word} can be
used as a synonym for string. For some alphabet $V$ we have $V^*$ as the language that contains
strings of all lengths over $V$ including the empty string, denoted as $\lambda$. Further, 
$V^+ = V^* - \{\lambda\}$. 

In defining a specific type of language known as \textit{regular languages}, we can use
\textit{regular expressions}. We define regular expressions in an iterative manner over an alphabet
$V$, as follows: (1) each $a \in V$ and $\emptyset$ are regular expressions, (2) if $E_1$ and $E_2$ are regular expressions, then $E_1 \cup E_2$,
$E_1 E_2$, and $E_1^*$ are also regular expressions. The language defined by a regular expression $E$ is denoted as $L(E)$. If $E_1$ is a regular
expression $E_1 = a$ where  $a \in V$, then the language $L(E_1)$ is $\{a\}$. The language defined by regular expression $\emptyset$ is the empty
language $\{\}$. Given regular expressions $E_1$ and $E_2$ and the languages they define, $L(E_1)$ and $L(E_2)$ respectively, the language
defined by regular expression $E_1 \cup E_2$ is  $L(E_1 \cup E_2) =L(E_1) \cup L(E_2)$, the language defined by regular expression $E_1 E_2$
is $L(E_1 E_2) = \{xy|x \in L(E_1) \text{ and } y \in L(E_2) \}$, and the language defined by regular expression $E_1^*$ is $L(E_1^*) = \{xy|
x \in L(E_1) \cup \{\lambda\} \text{ and } y \in L(E_1^*)\}$. 


% ================================================================================================= %
\subsection{Spiking Neural P Systems}\label{sec-prelim-snp}
% ================================================================================================= %
\cite{chen-2008-snp-e}

\begin{definition2}[Spiking Neural P Systems]
A \emph{spiking neural P system}  of degree $n\geq1$ is a construct of the form $$\Pi = (O,\sigma_1,
...,\sigma_m, syn, in,out)$$ where
\begin{itemize}
   \item $O=\{a\}$ is the singleton \emph{alphabet} where the element $a$ is called a \emph{spike}.
   \item $\sigma_1,...,\sigma_m$ are \emph{neurons} having the form $\sigma_i=(n_i,R_i)$ for 
         $1 \leq i \leq n$ where:
         \begin{itemize}
            \item $n_i \geq 0$ is the \emph{initial number of spikes} in $\sigma_i$.
            \item $R_i$ is the finite \emph{rule set} of $\sigma_i$. A rule in $R_i$ has the form
                  $E/a^c \ra a^p\sm d$ where $E$ is a regular expression over $O$, $c\geq 1$, $p\geq0$,
                  $c\geq p$, and $d\geq 0$. $c$ is the number of spikes \emph{consumed} by the rule, 
                  $p$ is the number of spikes \emph{produced} by the rule, and $d$ is the spiking 
                  \emph{delay}.
         \end{itemize}
   \item $syn \subseteq \{1,2,...,m\} \times \{1,2,...,m\}$ with $(i,i) \notin syn$ for any
         $i \in \{1,2,...,m\}$ is the \emph{set of synapses} between neurons.
   \item $in,out \in \{1,2,...,m\}$ indicate the \emph{input} and \emph{output} neurons. 
\end{itemize} 
\end{definition2}

Let $E/a^c \ra a^p\sm d$ be a rule in neuron $\sigma_i$ (the rule is in $R_i)$. The rule works in the 
following manner. If neuron $\sigma_i$ has $k$ spikes, $a^k \in L(E)$, and $k \geq c$ then the rule
is \emph{applicable}. If $E/a^c \ra a^p\sm d$ is applicable and it is applied by neuron $\sigma_i$ at 
time $t$, then $c$ spikes are removed from neuron $\sigma_i$ at time $t$ (changing neuron
$\sigma_i$'s spike count to $k-c$) then $p$ spikes are sent to all neurons $\sigma_j$ where that 
$(i,j)\in syn$ at time $t+d$ (each neuron $\sigma_j$ receives $p$ spikes). At time $t$ to $t+d-1$ 
neuron $\sigma_i$ is said to be \emph{closed} which means the neuron can not receive spikes from 
other neurons. Spikes sent to a closed neuron are removed from the system. The rule is said to be
\emph{active} from time $t$ to $t+d$ which means no other rules in neuron $\sigma_i$ can be applied.

We say that rule $E_1/a^{c_1}\ra a^{p_1}\sm d_1$ and rule $E_2/a^{c_2}\ra a^{p_2}\sm d_2$ \emph{intersect}
if $L(E_1) \cap L(E_2) \neq \emptyset$. If those two rules are in neuron $\sigma_i$ with $k$ spikes,
$a^k \in L(E_1) \cap L(E_2)$, $k\geq c_1$, and $k\geq c_2$ then those two rules are applicable at
the same time. If a neuron has multiple applicable rules, then it non-deterministically selects one
rule to apply.

Given a rule $E/a^c \ra a^p\sm d$, if $E = a^c$, we can write the rule as $a^c \ra a^p\sm d$ . If $d=0$, 
we can write the rule as $E/a^c \ra a^p$. If $E=a^c$ and $d=0$, we can write the rule as $a^c \ra 
a^p$. If $p=0$, the rule is called a \emph{forgetting rule}, if $p=1$, the rule is called a 
\emph{standard spiking rule}, and if $p>1$, the rule is called an \emph{extended spiking rule}.

The original SNP system \cite{ionescu-2006-snp} only uses forgetting and standard spiking rules.
A forgetting rule in the original SNP system has the form $a^c \ra a^0$, written as $a^c\ra\lambda$,
which means the regular expression of the forgetting rule is always $E=a^c$ and it has no delay
($d=0$). Additionally, the original SNP system has the restriction that in a neuron no forgetting 
rules should intersect with any spiking rule. The SNP system in \cite{chen-2008-snp-e} introduces
the idea of the extended spiking rule but the actual SNP system model in \cite{chen-2008-snp-e} does
not use the concept of delay. 

The system assumes a global clock. The system operates in the following manner: for each step, each 
neuron in the system that does not have an active rule will check if any of its rules is applicable 
and if at least one rule is applicable then the neuron will apply an applicable rule. This mode of
operation is called \emph{minimally parallel}. i.e. Neurons work in parallel but each neuron only
applies only one applicable rule if there are any (a neuron has to apply one rule if there are any
applicable rules). The system will continue this operation until it reaches a halting state. The 
system is in a \emph{halting state} if all neurons in the system have no applicable rules and no
active rules. 

A configuration of the system is defined as the tuple $ C = (n_1/d_1,...,n_i/d_i,$ $...,n_m/d_m)$ 
where $n_i$ is the number of spikes in neuron $\sigma_i$ and $d_i \in \mathbb{N} \cup \{-1\}$ is a
number that represents the state of neuron $\sigma_i$. State $d_i=-1$ means the neuron $\sigma_i$ 
has no active rules. State $d_i=0$ means neuron $\sigma_i$ is open but has an active rule that is 
about to send spikes. State $d_i > 1$ means neuron $\sigma_i$ is closed and will only open and send 
spikes after $d_i$ steps. 

Given the configuration $C=(n_1/d_1,...,n_i/d_i,$ $...,n_m/d_m)$ we can know what events will occur 
and can occur in the system. An \emph{event} is either a neuron spiking, a neuron applying a rule, 
or a neuron `counting down' before spiking. An event changes a configuration. Neuron spiking sends 
out spikes to other neurons changing their spike counts, a neuron applying a rule consumes spikes, 
while a neuron with active rule `counting down' decrements the state $d$ of the neuron. If the 
component $n_i/d_i$ of $C$ has $d_i=0$, then neuron $\sigma_i$ spiking is an event in $C$. If the 
component $n_i/d_i$ has $d_i > -1$, then neuron $\sigma_i$ counting down is an event in $C$. If the 
component $n_i/d_i$ has $d_i=-1$ and neuron $\sigma_i$ has some applicable rule $r$, then neuron 
$\sigma_i$ applying rule $r$ is a possible event in $C$. We say that a set of events $S$ is
\emph{consistent with configuration $C$} if it includes all neuron spiking events and neuron 
countdown events in $C$ and it includes one rule application event for each neuron with no active 
rules but has some applicable rules. We say that configuration $C$ \emph{transitions} to 
configuration $C'$, written as $C \Rightarrow C'$, if $S$ is a set of events consistent with $C$ and
$C'$ is the result events in $S$ happening in $\Pi$ while its configuration is $C$. A 
\emph{computation} of $\Pi$ is simply a sequence of configuration transitions starting from the 
system's initial configuration $(n_1/\mn 1,n_2/\mn 1,...,n_m/\mn 1)$.

SNP systems can be used as an \emph{acceptor}, as a \emph{generator}, or as a \emph{transducer}. In
general, SNP systems use \emph{spike trains} as input and/or output. A spike train is simply a 
sequence of spikes which can also be interpreted as a sequence of spike counts. For example, the
spike train $a^3a^0a^2a^1$ is the sequence that starts with $3$ spikes, followed by $0$ spikes, then
by $2$ spikes and the by $1$ spike. As a sequence of spike count, the spike train $a^3a^0a^2a^1$
can be written as $3,0,2,1$. As an acceptor, the systems receives a spike train input in its input 
neuron. The spike train is accepted if the system halts. An acceptor SNP system computes the set of 
spike trains it accepts. As a generator, the system produces an output spike train via its output 
neuron. If the system halts, the spike train produced by the output neuron is the spike train 
generated by the system. A generator SNP system computes that set of spike trains it generates. As
a transducer, the system receives a spike train $x$ in its input neuron. The system either 
it halts with spike train $x$ as input or it does not. If the system halts, the spike train $y$
generated by the output neuron will be the output of the system for input $x$. The transducer system
computes some binary relation that contains the spike train pairs $(x,y)$ where $x$ is the input
spike train and the system halts on $x$ with output $y$.

% ================================================================================================= %
\section{Homogenization of Spiking Neural P Systems} \label{sec-homo}
% ================================================================================================= %


% ================================================================================================= %
\subsection{Representing Neurons as Transition Systems}\label{sec-homo-sm}
% ================================================================================================= %


%===================================================================================================

\begin{definition2}[Neuron Transition System] \label{def-nts}
A \emph{neuron transition system} (NTS) is a tuple $(S,V, \ra)$ where
\begin{itemize}
   \item $S$ is a finite \emph{set of states}. A \emph{state} $s \in S$ is a subset of $\mathbb{N}$.
         A state represents a set of spike counts.
   \item $V$ is a finite \emph{set of events}. An \emph{event} $e\in V$ has the form $(\alpha,\beta)$
         where $\alpha \in \mathbb{Z}$, $\beta \in \mathcal{A}$, and $\mathcal{A}$ is some 
         \emph{set of rule actions}. i.e. $\beta=\lambda$ (forgetting action), $\beta=a^p\sm d$ 
         spiking action with delay). If $\alpha < 0$, then the event $(\alpha,\beta)$ represents the
         application of a rule that consumes $\alpha$ spikes and performs action $\beta$. i.e. 
         $E/a^{\alpha} \ra \beta$. If $\alpha > 0$, then the event $(\alpha,\beta)$ represents the
         reception of $\alpha$ spikes and its $\beta$ can be set as a \emph{non-action}. i.e. 
         $\beta = \lambda$ (non-action, `action' of the forgetting rule). 
   \item $\ra \subseteq S \times V \times S$ is the \emph{transition relation}. A $(s,e,s')\in 
         \ra$ is called a \emph{transition}. If $e=(\alpha,\beta)$, the transition $(s,(\alpha,
         \beta),s')$ has the property $s'=\{n+\alpha\se|\se n \in s\}$. Since the next state $s'$
         can be derived from the current state $s$ and $\alpha$, the transition 
         $(s,(\alpha,\beta),s')$ can simply be written as $(s,(\alpha,\beta))$.
\end{itemize} 
\end{definition2}

%===================================================================================================

Let $nts = (S,V,\ra)$ be the NTS of some neuron $\sigma_w=(n,R)$. $nts$ is constructed using the 
rule set $R$ and the possible spike trains going to neuron $w$. For each rule $E/a^{\alpha} \ra 
\beta$ in $R$, the transition $(N(L(E)),(-\alpha, \beta))$ is an element of the transtion relation 
$\ra$. $N(L(E))$ is the length set of language $L(E)$. We will call the set of neurons $x$ 
\emph{connected to} neuron $w$, i.e. $(x,w) \in syn$, as the \emph{subsystem connected to} neuron 
$w$. If it is possible for the subsystem connected to neuron $w$ to send $\alpha$ spikes to neuron
$w$ while neuron $w$ is \emph{in state $s$} (neuron $w$ has $n$ spikes and $n\in s$), then the 
transition $(s,(\alpha,\lambda))$ is in the transition relation $\ra$. If the transition 
$(s,(\alpha,\lambda)) \in \ra$ represents neuron $w$ receiving $\alpha$ spikes when neuron $w$ is in 
state  $s$ and the transition $(s,(\alpha',\beta)) \in \ra$ represents an application of a rule that
consumes $\alpha'$ spikes and has action $\beta$ when neuron $w$ is in state $s$, then the
transition $(s,(\alpha+\alpha',\beta))$ is also an element of $\ra$. The event $(\alpha+\alpha',
\beta)$ actually represents two events, the reception of $\alpha$ spikes and the application of a 
rule that consumes $\alpha'$ spikes and has action $\beta$. 

%===================================================================================================


\begin{figure}[H]
   \centering
   \begin{subfigure}{.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-1a.pdf}
      \caption{}
      \label{fig-lts-1a}
   \end{subfigure}
   \begin{subfigure}{.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-1b.pdf}
      \caption{}
      \label{fig-lts-1b}
   \end{subfigure}
   \caption{Examples of Neurons and their NTSs}
   \label{fig-lts-1}
\end{figure}

%===================================================================================================

\begin{figure}[H]
   \centering
   \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-2a.pdf}
      \caption{}
      \label{fig-lts-2a}
   \end{subfigure}
   \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-2b.pdf}
      \caption{}
      \label{fig-lts-2b}
   \end{subfigure}
   \caption{Neurons with same rule set but different NTSs}
   \label{fig-lts-2}
\end{figure}

%===================================================================================================
\subsection{Operations on Neuron Transition Systems}\label{sec-homo-ops}
%===================================================================================================

We will define two operations on labelled transition systems, \emph{LTS translation} and \emph{LTS
scaling}. The idea behind these operations is that they take an LTS and a parameter $\delta$ and 
produce a new LTS that behaves exactly like the original LTS. When you are combining two different
rule sets from two different neurons in order to have common rule set, simply getting the union of
the two rule sets can cause conflicts. For example, let $\{a \ra a, a^2/a\ra a\}$ be the rule set 
of neuron $x$ and $\{a\ra\lambda,a^2/a\ra a\}$ be the rule set of neuron $y$. If we simply combine
the rule sets to have the common rule set $\{a\ra\lambda,a\ra a, a^2/a\ra a\}$ for both neurons
$x$ and $y$, there will be an unwanted behavior. If neuron $x$ has $1$ spike it
should use the rule $a\ra a$ and if neuron $y$ has $1$ spike it should use the rule $a\ra \lambda$.
If neurons $x$ and $y$ use the common rule set, when they have $1$ spike both of them will have
a non-deterministic choice, use rule $a\ra \lambda$ or use rule $a\ra a$. This non-determinism is 
an unwanted behavior that is the result of rule $a\ra\lambda$ `conflicting' with rule $a\ra a$. The 
common rule set changes the behavior of both neuron $x$ and neuron $y$. LTS translation and LTS 
scaling will be used to avoid such conflicts and changes in the neurons' behavior when combining
different rule sets. 

\begin{definition2}[State Translation] 
\emph{State translation} is an operation on a state. It takes a state $s$ and a natural number 
$\delta$ and produces the state $s'$ defined as $s' = \{n + \delta\se|\se n \in s\}$. We
denote state translation with the $+$ symbol. i.e. $s+\delta = \{n + \delta\se|\se n \in s\}$.
We say that $s'$ is \emph{$s$ translated by $\delta$}.
\end{definition2}

For example, if $s_1=\{3,5,7\}$ and $\delta_1 = 3$, then $s_1+\delta_1 = s_1+3 = \{6,8,10\}$. If
$s_2 = \{3,6,9, 12,...\}= \{3i\}_{i\geq 1}$ and $\delta_2 = 2$, then $s_2+\delta_2 = s_2+2 = 
\{5,8,11,14,...\}$ $=\{3i+2\}_{i\geq 1}$. If $s_3 = \{1,3,5,7,...\} =\{2i-1\}_{i\geq 1}$ and 
$\delta_3=1$, then $s_3+\delta_3 = s_3 + 1 = \{2,4,6,8,..\}=\{2i\}_{i\geq 1}$.

\begin{definition2}[Transition Translation]
\emph{Transition translation} is an operation on a transition. It takes a transition $t = (s,
(\alpha,\beta))$ and a natural number $\delta$ and produces the transition $t'=(s',(\alpha,i
\beta))$ where $s'=s+\delta$ (state $s$ is translated by $\delta$). We also will use the same symbol 
$+$ for transition translation. i.e. $t' = t +\delta$. We say that $t'$ is \emph{$t$ translated by
$\delta$}.
\end{definition2}

A translation of the transition $(s,(\alpha,\beta))$ simply involves the translation of the state
$s$ component. The label component $(\alpha,\beta)$ is not modified. Since some transitions 
represent rules, the change due to transition translation has a corresponding change to the rule
that transition represents. If transition is translated by $\delta$, the regular expression $E$
of the rule is also translated by $\delta$. i.e. $a^{\delta}E$ is $E$ translated by $\delta$. For
example, if $E=a(a^2)^*$ then $E$ translated by $\delta$ is $a^{\delta}a(a^2)^*=a^{\delta+1}
(a^2)^*$. Let $r$ be the rule $E/a^c \ra \beta$ (where $\beta$ is some action e.g. spiking,
forgetting, spiking with delay, etc). We say some rule $r'$ is rule $r$ \emph{translated by 
$\delta$} if $r'$ is the rule $a^{\delta} E/a^c \ra \beta$. We will call this operation 
\emph{rule translation} which is the exact analogue of transition translation.

\begin{definition2}[LTS Translation]
\emph{LTS translation} is an operation on an entire transition system. It takes a labelled 
transition system $lts=(S,L,\ra)$ and a natural number $\delta$ and produces the labelled transition 
system $lts'=(S',L,\ra')$ where $S'=\{s+\delta\se|\se s \in S\}$ and $\ra' = \{t+\delta\se|\se t \in 
\ra\}$. We also use the same symbol $+$ for LTS translation. i.e. $lts'=lts+\delta$. We say that 
$lts'$ is \emph{$lts$ translated by $\delta$}.
\end{definition2}

The translation of $lts=(S,L,\ra)$ simply involves the translation of all transitions in the 
relation $\ra$ by the same amount $\delta$. This implies a change of the set of states $S$. The set 
of labels $L$ is unchanged.  

When you translate a neuron LTS, the translated LTS represents the same neuron behavior as the 
original LTS but the translated LTS is for a different neuron. The translated LTS is for the 
\emph{translated neuron}. Let some neuron $w$ have an initial number of spike $n$. We say some 
neuron $w'$ is neuron $w$ \emph{translated by} $\delta$ if neuron $w'$ has $n+\delta$ initial number 
of spikes and contains $\delta$-translated rules of neuron $w$. We will call this operation 
\emph{neuron translation} which is the exact analogue of LTS translation. Figure \ref{fig-lts-3} 
shows neuron $w'$ which is a $\delta$-translated neuron $w$ from Figure \ref{fig-lts-1a}. It also
shows the LTS of neuron $w'$ which is a $\delta$-translated LTS of neuron $w$ from Figure
\ref{fig-lts-1a}. Neurons $w$ and $w'$ have the same behavior because for any sequence of events
(sequence of receptions of spikes) they will have the same sequence of actions (sequence of applied 
rules).

%===================================================================================================

\begin{figure}[H]
   \centering
   \includegraphics[scale=0.70]{fig-lts-3.pdf}
   \caption{Neuron $w'$ and its LTS}
   \label{fig-lts-3}
\end{figure}

\begin{lemma}
A neuron and a translated version of the neuron have the same behavior.
\end{lemma}

\begin{proof}
Let neuron $w'$ with LTS $lts'$ be a $\delta$-translated version of some neuron $w$ with LTS $lts$. 
If neuron $w$ has an initial $n_0$ spikes, then neuron $w'$ has an initial $n_0+\delta$ spikes. Let 
$T_0$ be the set of transition in $lts$ such that $(s,(\alpha,\beta))\in T_0$ if and only if 
$n_0\in s$. Let $T'_0$ be the corresponding set for $lts'$. i.e. $T_0'=\{(s',(\alpha',\beta'))\se|\se
n_0+\delta \in s'\}$. If $(s,(\alpha,\beta))\in T_0$, then $(s+\delta,(\alpha,\beta)) \in T'_0$ since
$n_0\in s$ and $n\in s$ implies $n_0+\delta \in s+\delta$. If  $(s+\delta,(\alpha,\beta)) \in T'_0$,
then $(s,(\alpha,\beta))\in T_0$ since $n_0+\delta \in s+\delta$ and $n_0+\delta \in s+\delta$ implies
$n_0\in s$ (by reversing the $\delta$-translation of state $s+\delta$). This means that the 
transitions in $T'_0$ are all $\delta$-translated transitions of $T_0$. Since transition translation
does not change the label $(\alpha,\beta)$, the transitions in $T_0$ has the same set of labels as
the transitions in $T'_0$. This means that neuron $w$ at $n_0$ spike count and neuron $w'$ at $n_0+
\delta$ have the same set of actions they can perform. The same argument can be used for when neuron
$w$ has spike count $n$ (which is not necessarily the initial spike count) while neuron $w'$ has
spike count $n+\delta$.
\end{proof}

%===================================================================================================

\begin{definition2}[State Scaling]
\emph{State scaling} is an operation on a state. It takes a state $s$ and a natural number
$\delta$ and produces the state $s'$ defined as $s' = \{\delta \cdot n\se|\se n\in s\}$. We
denote state scaling with the $\cdot$ symbol. i.e. $s'=\delta \cdot s$. We say that $s'$ is 
\emph{$s$ scaled by $\delta$}.
\end{definition2}

For example, if $s_1=\{1,2,3\}$ and $\delta_1=5$, then $\delta_1\cdot s_1 = 5\cdot s_1=\{5,10,15\}$.
If $s_2=\{2,4,6,8,...\}=\{2i\}_{i\geq 1}$ and $\delta_2=2$, then $\delta_2\cdot s_2=2\cdot s_2 =
\{4,8,12,16,...\}=\{4i\}_{i\geq 1}$. If $s_3=\{4,7,10,13,...\}=\{3i+1\}_{i\geq 1}$ and $\delta_3=2$,
then $\delta_3\cdot s_3 = 2\cdot s_3 = \{8,14,20,26,...\}=\{6i+2\}_{i\geq1}$.

\begin{definition2}[Transition Scaling]
\emph{Transition scaling} is an operation on a transition. It takes a transition $t=(s,(
\alpha,\beta))$ and a natural number $\delta$ and produces the transition $t'=(s',(\alpha',
\beta))$ where $s' = \delta \cdot s$ and $\alpha' = \delta \cdot \alpha$. We also use the $\cdot$
symbol for transition scaling. i.e. $t'=\delta\cdot t$. We say that $t'$ is \emph{$t$ scaled by 
$\delta$}.
\end{definition2}

Transition scaling scales both the state component $s$ and the $\alpha$ component (number of spikes
consumed or received) of the label $(\alpha,\beta)$ by the same factor $\delta$. Since some 
transitions represent rules, similar to transition translation there is a rule analouge for 
transition scaling which we will call \emph{rule scaling}. If $r$ is the rule $E/a^c \ra \beta$,
$r'$ is rule $r$ \emph{scaled by $\delta$}, written as $r'=\delta \cdot r$, if it is the rule
$E'/a^{\delta \cdot c} \ra \beta$ where $E'$ is the modified expression $E$ where all instances of 
expression $a$ is replace by expression $a^\delta$. For example, if $r$ is the rule $a(a^3)^*/a^2 \ra 
a$ with the corresponding transition $(\{3i+1\}_{i\geq 0},(-2,1))$ then
$r'=\delta \cdot r$ is the rule $(a^{\delta})((a^{\delta})^3)^*/a^{2\delta}\ra a$, or simply
$a^{\delta}(a^{3\delta})^*/a^{2\delta}\ra a$, with the corresponding transition $(
\{3i\delta+\delta\}_{i\geq 0},(-2\delta,1))$.

\begin{definition2}[LTS Scaling]
\emph{LTS scaling} is an operation on an entire transition system. It takes a labelled transition 
system $lts=(S,L,\ra)$ and a natural number $\delta$ and produces the labelled transition
system $lts'=(S',L',\ra')$ where $S' = \{\delta \cdot s\se|\se s\in S\}$, $L'=\{(\delta\cdot\alpha,
\beta\se)|\se (\alpha,\beta)\in L\}$, and $\ra' = \{\delta\cdot t\se|\se t \in \ra\}$. We also 
use the $\cdot$ symbol for LTS scaling. i.e. $lts'=\delta\cdot lts$. We say that $lts'$ is 
\emph{$lts$ scaled by $\delta$}.
\end{definition2}

%===================================================================================================

\begin{figure}[H]
   \centering
   \begin{subfigure}{.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-4a.pdf}
      \caption{}
      \label{fig-lts-4a}
   \end{subfigure}
   \begin{subfigure}{.49\textwidth}
      \centering
      \includegraphics[scale=0.60]{fig-lts-4b.pdf}
      \caption{}
      \label{fig-lts-4b}
   \end{subfigure}
   \caption{Two ways of scaling NTSs of neuron $w$ and its connected subsyste}
   \label{fig-lts-4}
\end{figure}

%===================================================================================================
\subsection{Procedures for Homogenizing Neurons' Rule Sets}\label{sec-homo-algo}
%===================================================================================================

SECTION INTRO PARAGRAPH

\begin{definition2}[Rule Transitions]
A rule set $R$ is a set of rules $\{r_1,...,r_i,...,r_n\}$ where each rule $r_i$ has the form 
$(s_i,a_i)$. For rule $r_i=(s_i,a_i)$, $s_i$ is called the state while $a_i$ is called action.
The \emph{scope} of a rule set $R=\{(s_1,a_1),...,(s_n,a_n)\}$, denoted by $scope(R)$, is the state 
$s = s_1 \cup s_2 \cup \cdots \cup s_n$.
\end{definition2}

SHORT INFO ON RULE SET, why rule set

\begin{definition2}[State-Actions Pair]
Given a rule set $R=\{(s_1,a_1),...,(s_n,a_n)\}$, we can associate a set of actions to any spike 
count $c$ in the scope of the rule set $R$. Let spike count $c$ be in the scope of $R$, $c\in 
scope(R)$, the actions associated with spike count $c$, denoted by $act(c)$, is the set 
$\{a_i\se |\se (s_i,a_i) \in R, c\in s_i\}$. We say that ``$act(c)$ is the set of actions at spike
count $c$". A set of actions can also be associated with a set of spike counts instead of simply
being associated with a single count. We say that set of spike counts $C$ is associated with actions
$act(C)$ if for all $c \in C$ the actions $act(c)$ at spike count $c$ is equal to $act(C)$. i.e. All
spike counts in $C$ have the same set of actions.
spike count,
\end{definition2}

MOTIVATION EXAMPLES

\begin{definition2}[Partition-Actions Set]
DEF
\end{definition2}


For example, $R = \{(s_1,a_1),(s_2,a_2),(s_3,a_3)\}$
$A = s_1\backslash (s_2 \cap s_3)$ 
$B = s_2\backslash (s_1 \cap s_3)$ 
$C = s_3\backslash (s_1 \cap s_2)$ 
$D = (s_1 \cup s_2 ) \backslash s_3$ 
$E = (s_1 \cup s_3 ) \backslash s_2$ 
$F = (s_2 \cup s_3 ) \backslash s_1$ 
$G = (s_1 \cup s_2 \cup s_3 )$ 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{fig-partitions-1.pdf}
    \caption{Partitions and Actions}
    \label{fig-partition-1}
\end{figure}

ELABORATE AND GIVE EXAMPLES(3)
%===================================================================================================

\begin{algorithm}[H]
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{$R = \{(s_1,a_1),...,(s_n,a_n)\}$}
\KwOutput{$P = \{(p_1,A_1),...,(p_m,A_m)\}$}
$P \leftarrow \{\}$\;
\For{each $T \subseteq R$}
{
   $S   \leftarrow \{s_k \se|\se (s_k,a_k) \in T\}$             \;
   $S'  \leftarrow \{s_l \se|\se (s_l,a_l) \in R\backslash T\}$ \;
   $A_i \leftarrow \{a_k \se|\se (s_k,a_k) \in T\}$             \; 
   $p_i \leftarrow \Big(\bigcap\limits_{s_k \in  S} s_k \Big)  
                   \Big\backslash 
                   \Big(\bigcup\limits_{s_l \in S'} s_l \Big)$   \;
   \If{$p_i \neq \varnothing$}
   {
      Add $(p_i, A_i)$ to $P$ \;
   }
}

\For{each $(p_x,A_x),(p_y,A_y) \subseteq P$}
{
   \If{$A_x = A_y$}
   {
      Remove $(p_x,A_x)$ and $(p_y,A_y)$ from $P$\;
      Add $(p_x \cup p_y, A_x)$ to $P$ \;
   }
}
\caption{$GetPartitionActionsSet(R):$ \\Get the Partition-Actions Set that corresponds to rule
        transition set $R$}
\end{algorithm}

%===================================================================================================

\begin{definition2}[Compatibility of Two Partition-Actions Sets] Two partition-actions sets, 
$P=\{(p_i,A_i)\}$ and $P'=\{(p'_j,A'_j)\}$, are compatible if for all $c \in scope(P) \cup 
scope(P')$ there is a $(p_i, A_i)\in P$ and a $(p'_j,A'_j) \in P'$  such that $c\in p_i$,
$c\in p'_j$, and $A_i=A'_j$.
\end{definition2}
%===================================================================================================

\begin{algorithm}[H]
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{$P = \{(p_1,A_1),...,(p_n,A_n)\}, P' = \{(p'_1,A'_1),...,(p'_m,A'_m)\}$}
\KwOutput{True or False}
$s  \leftarrow scope(P)$\;
$s' \leftarrow scope(P')$\;
$P  \leftarrow \{(p_1\cap s', A_1),...,(p_n\cap s', A_n)\}$\;
$P' \leftarrow \{(p'_1\cap s, A'_1),...,(p'_n\cap s, A'_m)\}$\;
\eIf{$P = P'$}
{
   return true;
}
{
   return false;
}
\caption{$Compatible(P,P')$\\
Return $true$ if partition-actions set $P$ is compatible with partition-actions set $P'$}
\end{algorithm}


%===================================================================================================
\begin{algorithm}[H]
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{$\{R_1,...,R_p\}$}
\KwOutput{$R_0$}
$R_0 \leftarrow \{\}$\;
\For{each $R \in \{R_1,...,R_p\}$}
{
   $R_0 \leftarrow Merge(R_0,R)$\;
}
return $R_0$\;
\caption{Homogenization Algorithm}
\end{algorithm}

%===================================================================================================
\begin{algorithm}[H]
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{$R_x,R_y$}
\KwOutput{$R_z$}
Convert rule set $R_x$ to partition-actions set $P_x$\;
Convert rule set $R_y$ to partition-actions set $P_y$\;
\eIf{$P_x$ is compatible with $P_x$}
{
   $R_Z \leftarrow R_x \cup R_y$\;
}
{
   Match $P_x$ and $P_y$\; 
   \For{each $(p_i,A_i) \in P_x$ and each $(p_j,A_j) \subseteq P_y$}
   {
   }
}
return $R_Z$\;
\caption{Merging Two Rule Sets}
\end{algorithm}


%===================================================================================================
\begin{algorithm}[H]
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{$(p,A=(\alpha,\beta)),(p',A'=(\alpha',\beta'))$}
\KwOutput{(matching parameters)/no match}

\eIf{$\beta \neq \beta'$}
{
   return `no match'\;
}
{
   Solve $u_1\alpha+v_1=u_2\alpha'+v_2$\;
   //This linear diophantine equation is solved using Euclidean GCD algorithm\;
}
return $(u_1,v_1,u_2,v_2)$\;
\caption{Matching of Two Partition-Actions Pairs}
\end{algorithm}

%===================================================================================================
\section{Remarks and Conclusions}\label{sec-rem-con}
%===================================================================================================

\bibliographystyle{elsarticle-harv}
\bibliography{homogeneous-snp}

%===================================================================================================



 
\end{document}
